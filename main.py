from fastapi import FastAPI, UploadFile, File, HTTPException, Form, Request
from faster_whisper import WhisperModel, BatchedInferencePipeline
from fastapi.middleware.cors import CORSMiddleware
import torch
import os
import uuid
import time
import shutil
import logging
import logging.handlers # ì¶”ê°€
import subprocess
import wave
import math
from pythonjsonlogger import jsonlogger # ì¶”ê°€

# --- FastAPI ì•± ìƒì„± ë° CORS ì„¤ì • ---
app = FastAPI(
    title="Whisper STT API Server",
    description="ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” API (Batched Inference Pipeline ì‚¬ìš©)",
    version="1.0.0"
)

# --- Global Exception Handler for Logging ---
from fastapi.responses import JSONResponse
from fastapi.requests import Request
from fastapi.exception_handlers import RequestValidationError
from fastapi.exceptions import RequestValidationError as FastAPIRequestValidationError
from starlette.exceptions import HTTPException as StarletteHTTPException

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error({
        "event": "unhandled_exception",
        "path": str(request.url),
        "error": str(exc),
        "type": type(exc).__name__
    })
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal Server Error"}
    )

@app.exception_handler(StarletteHTTPException)
async def http_exception_handler(request: Request, exc: StarletteHTTPException):
    logger.error({
        "event": "http_exception",
        "path": str(request.url),
        "error": str(exc.detail),
        "status_code": exc.status_code
    })
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail}
    )

@app.exception_handler(FastAPIRequestValidationError)
async def validation_exception_handler(request: Request, exc: FastAPIRequestValidationError):
    logger.error({
        "event": "validation_exception",
        "path": str(request.url),
        "errors": exc.errors(),
        "body": await request.body()
    })
    return JSONResponse(
        status_code=422,
        content={"detail": exc.errors()}
    )

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ì„¤ì • ---
MODEL_SIZE = "medium"
if torch.cuda.is_available():
  DEVICE = "cuda"
  COMPUTE_TYPE = "float16"
else:
  DEVICE = "cpu"
  COMPUTE_TYPE = "int8"

UPLOAD_DIR = "temp_audio" # ì—…ë¡œë“œëœ ì˜¤ë””ì˜¤ ì„ì‹œ ì €ì¥ í´ë”
LOGS_DIR = "logs"         # ë¡œê·¸ íŒŒì¼ ì €ì¥ í´ë”
SERVICE_NAME = "whisper-stt-server" # ELKì—ì„œ ì´ ì„œë¹„ìŠ¤ ì‹ë³„ ì´ë¦„
BATCH_SIZE = 16
NUM_WORKERS = min(4, os.cpu_count() or 4)


# --- ë¡œê¹… ì„¤ì • ---
logger = logging.getLogger(SERVICE_NAME)
logger.setLevel(logging.DEBUG) # ê°œë°œ ì‹œì—ëŠ” DEBUG, ì‹¤ì œ ìš´ì˜ ì‹œ INFO ë“±ìœ¼ë¡œ ì¡°ì •
logger.propagate = False # ë£¨íŠ¸ ë¡œê±°ë¡œì˜ ì „íŒŒ ë°©ì§€ (ì¤‘ë³µ ë¡œê¹… ë°©ì§€)

# ë¡œê·¸ í´ë” ìƒì„±
os.makedirs(LOGS_DIR, exist_ok=True)
LOG_FILE_PATH = os.path.join(LOGS_DIR, "whisper_server.log")

# JSON í¬ë§·í„°
class CustomJsonFormatter(jsonlogger.JsonFormatter):
    def add_fields(self, log_record, record, message_dict):
        super(CustomJsonFormatter, self).add_fields(log_record, record, message_dict)
        if not log_record.get('@timestamp'): # ì´ë¯¸ @timestampê°€ ìˆë‹¤ë©´ ì‚¬ìš© (Filebeat ë“±ì—ì„œ ì„¤ì • ê°€ëŠ¥)
            log_record['@timestamp'] = logging.Formatter().formatTime(record, datefmt='%Y-%m-%dT%H:%M:%S.%fZ')
        if record.levelname:
            log_record['log.level'] = record.levelname.upper()
        else:
            log_record['log.level'] = 'INFO' # ê¸°ë³¸ê°’
        log_record['service.name'] = SERVICE_NAME
        # transaction.idëŠ” ë¡œê¹… í˜¸ì¶œ ì‹œ extraë¡œ ì „ë‹¬ë°›ì•„ ìë™ìœ¼ë¡œ í¬í•¨ë¨

# í¬ë§·í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
formatter = CustomJsonFormatter()

# íŒŒì¼ í•¸ë“¤ëŸ¬ (RotatingFileHandler)
file_handler = logging.handlers.RotatingFileHandler(
    LOG_FILE_PATH, maxBytes=10*1024*1024, backupCount=5, encoding='utf-8'
)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# ì½˜ì†” í•¸ë“¤ëŸ¬ (ê°œë°œ ì‹œ í™•ì¸ìš© - JSON í¬ë§·í„° ë™ì¼í•˜ê²Œ ì ìš©)
stream_handler = logging.StreamHandler()
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler) # ê°œë°œ ì¤‘ì—ëŠ” ì½˜ì†” ì¶œë ¥ë„ ê°™ì´ ë³´ë©´ í¸í•©ë‹ˆë‹¤.

# --- ê¸°ì¡´ logging.basicConfig(level=logging.INFO) ë¶€ë¶„ì€ ì‚­ì œ ë˜ëŠ” ì£¼ì„ ì²˜ë¦¬ ---
# logging.basicConfig(level=logging.INFO) # ì´ ì¤„ì€ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì‚­ì œí•©ë‹ˆë‹¤.


# --- ëª¨ë¸ ë¡œë“œ ---
# ëª¨ë¸ ë¡œë“œ ì‹œì ì— ëŒ€í•œ ë¡œê·¸ (ê¸°ë³¸ ì •ë³´ í¬í•¨)
base_log_extra_model_load = {"event.module": "initialization", "event.action": "load_model"}
logger.info(
    f"Attempting to load Whisper model '{MODEL_SIZE}' on device '{DEVICE}' with compute_type '{COMPUTE_TYPE}'",
    extra={**base_log_extra_model_load, "model.size": MODEL_SIZE, "model.device": DEVICE, "model.compute_type": COMPUTE_TYPE}
)
try:
  base_model = WhisperModel(
    MODEL_SIZE,
    device=DEVICE,
    compute_type=COMPUTE_TYPE,
    cpu_threads=NUM_WORKERS,
  )
  model = BatchedInferencePipeline(model=base_model)
  logger.info("Whisper model with batched pipeline loaded successfully.", extra=base_log_extra_model_load)
except Exception as e:
  logger.critical(f"CRITICAL: Error loading Whisper model. Application may not work correctly.", exc_info=True, extra=base_log_extra_model_load)
  model = None
  base_model = None


# --- ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ ---
def process_audio(audio_path: str, request_id: str, batch_size: int = BATCH_SIZE, language: str = None):
    """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í™œìš©í•´ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜. ìš”ì²­ IDë¥¼ ë°›ì•„ ë¡œê¹…ì— í™œìš©."""
    # ì˜¤ë””ì˜¤ ê¸¸ì´(ì´ˆ) ê³„ì‚°
    try:
        with wave.open(audio_path, 'rb') as wf:
            audio_duration_sec = wf.getnframes() / wf.getframerate()
    except Exception:
        audio_duration_sec = None

    log_extra_base = {"transaction.id": request_id, "audio.path": audio_path, "stt.batch_size": batch_size, "audio.duration_sec": round(audio_duration_sec, 2) if audio_duration_sec else "N/A"}
    logger.info("Starting audio processing (transcription)", extra=log_extra_base)
    start_time = time.time()

    try:
        transcription_options = {
            "beam_size": 5, "vad_filter": True, "word_timestamps": True,
            "condition_on_previous_text": True, "batch_size": batch_size,
            "task": "transcribe", "best_of": 5, "temperature": 0,
            "initial_prompt": "This is a business meeting recording."
        }
        # ğŸ”½ ì´ ì•„ë«ë¶€ë¶„ì´ ì´ì „ ì œ ì½”ë“œì—ì„œ ì œê³µëœ ìƒì„¸ ë¡œê¹… ë° STT ì²˜ë¦¬ ë¡œì§ì…ë‹ˆë‹¤.
        # ì´ ë¶€ë¶„ì„ ì±„ì›Œì£¼ì‹œë©´ ë©ë‹ˆë‹¤.

        log_extra_transcribe = {**log_extra_base, "stt.options_preview": {k:v for k,v in transcription_options.items() if k != "initial_prompt"}} # í”„ë¡¬í”„íŠ¸ëŠ” ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œì™¸ ë˜ëŠ” ì¼ë¶€ë§Œ

        if language:
            transcription_options["language"] = language
            log_extra_transcribe["stt.language.user_specified"] = language
            if language == "ko":
                transcription_options["initial_prompt"] = "This is a Korean business meeting recording. The primary language is Korean with occasional English terms or phrases. Please accurately transcribe the Korean speech while maintaining English terms that might be present. Focus on properly capturing Korean sentences with their natural flow and structure."
                # í”„ë¡¬í”„íŠ¸ ë‚´ìš©ë„ ì¤‘ìš” ì •ë³´ì´ë¯€ë¡œ ë¡œê¹… (ê¸¸ë‹¤ë©´ ì¼ë¶€ë§Œ)
                log_extra_transcribe["stt.initial_prompt_used"] = transcription_options["initial_prompt"][:100] + "..." if len(transcription_options["initial_prompt"]) > 100 else transcription_options["initial_prompt"]
                logger.info("Transcribing with specific Korean language settings and prompt", extra=log_extra_transcribe)
            else:
                log_extra_transcribe["stt.initial_prompt_used"] = transcription_options["initial_prompt"][:100] + "..." if len(transcription_options["initial_prompt"]) > 100 else transcription_options["initial_prompt"]
                logger.info(f"Transcribing with specified language: {language}", extra=log_extra_transcribe)
        else:
            log_extra_transcribe["stt.initial_prompt_used"] = transcription_options["initial_prompt"][:100] + "..." if len(transcription_options["initial_prompt"]) > 100 else transcription_options["initial_prompt"]
            logger.info("Transcribing with automatic language detection", extra=log_extra_transcribe)

        # model ê°ì²´ê°€ Noneì´ ì•„ë‹Œì§€ í™•ì¸ (ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ë¡œë“œ ì‹¤íŒ¨ ê²½ìš° ëŒ€ë¹„)
        if model is None:
            logger.error("Whisper model is not loaded. Cannot perform transcription.", extra=log_extra_base)
            raise RuntimeError("Whisper model not available for transcription.")


        segments_iterable, info = model.transcribe(audio_path, **transcription_options)
        segments_list = list(segments_iterable) # ì œë„ˆë ˆì´í„° ì†Œëª¨ ë° ê²°ê³¼ ë¡œê¹…ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ ë³€í™˜

        # ìµœì¢… í…ìŠ¤íŠ¸ í†µê³„
        transcript_word_count = sum(len(segment.text.strip().split()) for segment in segments_list)

        end_time = time.time()
        processing_time_sec = end_time - start_time

        final_log_extra = {
            **log_extra_base,
            "stt.processing_time_sec": round(processing_time_sec, 2),
            "stt.language.detected": info.language if info else "N/A",
            "stt.language.probability": round(info.language_probability, 4) if info else "N/A",
            "stt.num_segments": len(segments_list),
            "transcript.word_count": transcript_word_count,
            "stt.throughput_ratio": round(processing_time_sec / audio_duration_sec, 2) if audio_duration_sec else "N/A",
            "stt.words_per_sec": round(transcript_word_count / processing_time_sec, 2) if processing_time_sec else "N/A",
        }
        logger.info("Transcription completed successfully", extra=final_log_extra)
        return segments_list, info # ì •ìƒ ì™„ë£Œ ì‹œ ê²°ê³¼ ë°˜í™˜

    except Exception as e:
        elapsed_time_sec = time.time() - start_time # ì—ëŸ¬ ë°œìƒ ì‹œì ê¹Œì§€ì˜ ì‹œê°„
        logger.error(
            "Error during STT batch processing",
            exc_info=True, # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í¬í•¨
            extra={**log_extra_base, "error.message_detail": str(e), "stt.processing_time_sec_before_error": round(elapsed_time_sec, 2)}
        )
        raise # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ FastAPIê°€ ì²˜ë¦¬í•˜ë„ë¡ í•¨ (ì˜ˆ: /upload-audio í•¸ë“¤ëŸ¬ì˜ except ë¸”ë¡)


# --- FastAPI ì—”ë“œí¬ì¸íŠ¸ ---
from typing import Optional

@app.post("/upload-audio")
async def upload_audio(request: Request, file: UploadFile = File(...), meeting_info: str = Form("N/A"), language: Optional[str] = Form(None)):
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ STTë¡œ ë³€í™˜í•˜ì—¬ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ JSON ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    import tempfile
    start_time = time.time()
    # ìš”ì²­ ID ê°€ì ¸ì˜¤ê¸°
    request_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
    # ì„ì‹œ íŒŒì¼ ì €ì¥
    os.makedirs(UPLOAD_DIR, exist_ok=True)
    suffix = os.path.splitext(file.filename)[-1] if file.filename else ".tmp"
    with tempfile.NamedTemporaryFile(delete=False, dir=UPLOAD_DIR, suffix=suffix) as temp_file:
        shutil.copyfileobj(file.file, temp_file)
        temp_path = temp_file.name

    # wav ë³€í™˜ (í•„ìš” ì‹œ)
    converted_wav_path = temp_path
    if not temp_path.lower().endswith(".wav"):
        converted_wav_path = temp_path + ".wav"
        try:
            subprocess.run([
                "ffmpeg", "-y", "-i", temp_path,
                "-ar", "16000", "-ac", "1", "-f", "wav", converted_wav_path
            ], check=True, capture_output=True)
        except Exception as e:
            os.remove(temp_path)
            raise HTTPException(status_code=500, detail=f"ffmpeg ë³€í™˜ ì‹¤íŒ¨: {e}")
        os.remove(temp_path)

    try:
        segments, info = process_audio(converted_wav_path, request_id, BATCH_SIZE, language)
        sorted_segments = sorted(segments, key=lambda s: s.start)
        full_text = "\n".join([segment.text.strip() for segment in sorted_segments])
        return {
            "text": full_text,
            "meeting_info": meeting_info,
            "processing_time_sec": round(time.time() - start_time, 2)
        }
    finally:
        if os.path.exists(converted_wav_path):
            os.remove(converted_wav_path)


@app.get("/")
async def read_root():
    """
    Root health check.
    """
    return {"message": "Whisper STT API Server with BatchedInferencePipeline is running"}